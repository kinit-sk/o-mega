{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# If you do some changes in file. jupiter notebook will update saved files \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import captum.attr as a\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from dataset import OurDataset\n",
    "from evaluate import EvaluateExplanation\n",
    "from explain import STS_ExplainWrapper, ExplanationExecuter_STS\n",
    "from hyperoptimalization import Hyper_optimalization\n",
    "from check_explanations import Compare_docano_XAI,Check_docano_XAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OurDataset(csv_dirpath=\"./data\")\n",
    "file='rationale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=Check_docano_XAI(rationale_dataset_path=f'../data/annotations/{file}.json',xai_dataset=dataset,tuple_indexes=dataset.fact_check_post_mapping) #data=doccano importance_map=xai\n",
    "indexes_doc, indexes_xai, doc_data=check.get_matched_doccano()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_dataset=round((0.2/100)*len(indexes_xai))\n",
    "indexes_xai=indexes_xai[:length_dataset]    \n",
    "dataset.fact_check_post_mapping = [dataset.fact_check_post_mapping[i] for i in indexes_xai]\n",
    "loader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### layer method ####\n",
    "model = STS_ExplainWrapper.setup_t5_transformer(\"../models/GTR-T5-FT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\envs\\env_autoxai_project\\lib\\site-packages\\captum\\attr\\_models\\base.py:191: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#### ordinary method ####\n",
    "model = STS_ExplainWrapper.setup_t5_transformer(\"../models/GTR-T5-FT\",interpretable_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = a.LayerGradientXActivation(model, layer=model.get_embedding_layer())\n",
    "method = a.InputXGradient(model)\n",
    "explain = ExplanationExecuter_STS(method, compute_baseline=False, visualize_explanation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you dont have explanation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create annotation list\n",
    "# if you want to evaluate in tokens you need to use an\n",
    "post_list= []\n",
    "mask_list= []\n",
    "for i in doc_data:\n",
    "    post_doccano,mask=Compare_docano_XAI.connect_rationale(i)\n",
    "    if isinstance(mask,list):\n",
    "        mask=torch.Tensor(mask)\n",
    "    mask_list.append(mask)\n",
    "    post_list.append(post_doccano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]c:\\Users\\Dell\\anaconda3\\envs\\env_autoxai_project\\lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: aopc_suff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: aopc_compr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: auprc_plau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: token_f1_plau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: token_iou_plau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 170.15it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluation = EvaluateExplanation(rationale_path=f\"./tmp/Jaro_574_human_ordinary_tokens.json\", verbose=True)\n",
    "final_metric, all_metrics = evaluation.evaluate(loader, explain,task='post_claim_matching',annotation=mask_list,explanation_maps_word=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': {'aopc_suff': 0.95308954, 'aopc_compr': 0.69198155},\n",
       " 'plausibility': {'auprc_plau': 0.6305212595623555,\n",
       "  'token_f1_plau': 0.46938775510204084,\n",
       "  'token_iou_plau': 0.30666666666666664},\n",
       " 'visualization': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have explanation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method='Saliency'\n",
    "normalization='without_normalize'\n",
    "method_param={'Saliency':{'parameters': {'abs': False}}}\n",
    "model_param={}\n",
    "\n",
    "explanations=Hyper_optimalization.load_explanations(f'./results/explain_multilingual_e5_model.json',method_param,model_param,method)\n",
    "comparing=Compare_docano_XAI(rationale_dataset=doc_data,importance_map=explanations)\n",
    "explanations=comparing.change_token_explanation(model,dataset,word_exp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: auprc_plau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:00<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: token_f1_plau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:00<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metric: token_iou_plau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:00<00:00, 14.68it/s]\n",
      "c:\\Users\\Dell\\Desktop\\Kinit\\AutoXAI\\notebooks\\../src\\evaluate.py:262: RuntimeWarning: Mean of empty slice.\n",
      "  faithfullness = np.array(list(all_metrics[\"faithfulness\"].values())).mean()\n",
      "c:\\Users\\Dell\\anaconda3\\envs\\projectXAI\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Need to put all methods from text file manually.\n",
    "# Also doesn't matter what method will be put into ExplanationExecuter \n",
    "\n",
    "evaluation = EvaluateExplanation(verbose=True,rationale_path=f'../data/annotations/{file}.json')\n",
    "final_metric, all_metrics = evaluation.evaluate(loader, explain,explanation_maps=explanations,method_name='Input X Gradient')  #tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2615896462751331"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': {'aopc_compr': 0.07188656, 'aopc_suff': 0.934923},\n",
       " 'plausibility': {'auprc_plau': 0.05932348928028465,\n",
       "  'token_f1_plau': 0.0,\n",
       "  'token_iou_plau': 0.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_autoxai_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
