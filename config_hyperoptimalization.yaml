Hyperoptimalization_parameters:
#  rationale_path: "./data/annotations/rationale.json"
 explanations_path: "./notebooks/tmp/test.json"
 task: 'text_classification' #  'post_claim_matching'
 dataset: "./notebooks/data"
 dataset: "llangnickel/long-covid-classification-data" 
 num_classes: 6
 columns: 
 - text
 - label
 perc: 1
 model_path: "intfloat/multilingual-e5-large"
 embeddings_module_name: "embeddings.word_embeddings"
 methods: ["GAE_Explain","Occlusion","Input X Gradient","Guided Backprop","Feature Ablation","Kernel Shap","Gradient Shap","Lime","Saliency","ConservativeLRP"]
 normalizations: ["without_normalize"]
 explanation_maps_token: True
 plausability_weight: 0.5
 faithfulness_weight: 0.5
 multiple_object: False

 model_param:
   Lime:
     similarity_func:
       function_name:
         - captum.attr._core.lime.get_exp_kernel_similarity_function
       parameters:
         distance_mode: ["cosine", "euclidean"]
         kernel_width: [450, 750]
     interpretable_model:
       function_name:
         - captum._utils.models.linear_model.SkLearnLasso
       parameters:
         alpha: [1.0e-19, 1.0e-25]
   GAE_Explain:
     implemented_method: true
     layers:
       module_path_expressions:
         - "hf_transformer.encoder.layer.*.attention.self.dropout"
   ConservativeLRP:
     implemented_method: true
     layers:
       store_A_path_expressions:
         - "hf_transformer.embeddings"
       attent_path_expressions:
         - "hf_transformer.encoder.layer.*.attention.self.dropout"
       norm_layer_path_expressions:
         - "hf_transformer.embeddings.LayerNorm"
         - "hf_transformer.encoder.layer.*.attention.output.LayerNorm"
         - "hf_transformer.encoder.layer.*.output.LayerNorm"
   Occlusion_word_level:
     implemented_method: true
 
 method_param:
   Lime:
     parameters:
       n_samples: [80, 90]
     token_groups_for_feature_mask: true
   Saliency:
     parameters:
       abs: [true, false]
   Occlusion:
     parameters:
       sliding_window_shapes:
         - [3, 1024]
         - [5, 1024]
       strides:
         - [1, 1024]
         - [1, 512]
     compute_baseline: true
   Gradient Shap:
     parameters:
       stdevs: [0.1, 0.9]
       n_samples: [10, 15]
     compute_baseline: true
   Kernel Shap:
     parameters:
       n_samples: [80, 90]
     compute_baseline: true
   Feature Ablation:
     token_groups_for_feature_mask: true
     compute_baseline: true
   Occlusion_word_level:
     parameters:
       regex_condition:
         - ""
         - ".,!?;:â€¦"
   Integrated Gradients:
     parameters:
       n_steps: [60, 40]

Optuna_parameters: 
 sampler: "TPESampler"
 n_trials: 14
 n_startup_trials: 4
 seed: 1000

Results_parameters:
 table_counter: './notebooks/results/count_tables/baseline_counter.csv'
 table_sampler: './notebooks/results/sampler_tables/baseline_sampler.csv' 
 visual_aopc: './notebooks/results/aopc_plots/'